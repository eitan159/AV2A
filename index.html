<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AV2A: Training-free, open-vocabulary audio-visual event perception with score-level early fusion and dynamic thresholds.">
  <meta name="keywords" content="Audio-Visual, Event Perception, Open-Vocabulary, CLIP, CLAP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds</title>

  <!-- Fonts & CSS -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
</head>
<body>

<!-- Minimal Navbar -->
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- Optional: add your homepage link or remove this block -->
      <!--
      <a class="navbar-item" href="https://your-homepage.example.com">
        <span class="icon"><i class="fas fa-home"></i></span>
      </a>
      -->
    </div>
  </div>
</nav>

<!-- Hero -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds
          </h1>

          <div class="is-size-4 has-text-weight-semibold" style="margin-top: 12px; margin-bottom: 16px; color: #363636;">
            CVPR 2025
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Eitan Shaar*<sup>1</sup>,</span>
            <span class="author-block">Ariel Shaulov*<sup>2</sup>,</span>
            <span class="author-block">Gal Chechik<sup>1,3</sup>,</span>
            <span class="author-block">Lior Wolf<sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Bar-Ilan University</span>
            <span class="author-block"><sup>2</sup>Tel-Aviv University</span>
            <span class="author-block"><sup>3</sup>NVIDIA</span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 8px; color: #666;">
            <span class="author-block">*Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.13693" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Optional: arXiv (update href or remove) -->
              <!--
              <span class="link-block">
                <a href="https://arxiv.org/abs/XXXX.XXXXX" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              -->

              <!-- Code -->
              <span class="link-block">
                <a href="https://github.com/eitan159/AV2A"
                   class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>

              <!-- BibTeX anchor -->
              <span class="link-block">
                <a href="#BibTeX" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-quote-left"></i></span>
                  <span>BibTeX</span>
                </a>
              </span>
            </div>
          </div>

        </div> <!-- column -->
      </div> <!-- columns -->
    </div> <!-- container -->
  </div> <!-- hero-body -->
</section>

<!-- Teaser (static image) -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered" style="margin-top: 12px;">
        Training-free, open-vocabulary audio-visual event perception with score-level early fusion and dynamic thresholds.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In the domain of audio-visual event perception, which focuses on the temporal localization and classification of events across distinct modalities (audio and visual), existing approaches are constrained by the vocabulary available in their training data. This limitation significantly impedes their capacity to generalize to novel, unseen event categories. Furthermore, the annotation process for this task is labor-intensive, requiring extensive manual labeling across modalities and temporal segments, limiting the scalability of current methods. Current state-of-the-art models ignore the shifts in event distributions over time, reducing their ability to adjust to changing video dynamics. Additionally, previous methods rely on late fusion to combine audio and visual information. While straightforward, this approach results in a significant loss of multimodal interactions.
            To address these challenges, we propose Audio-Visual Adaptive Video Analysis (AV²A), a model-agnostic approach that requires no further training and integrates an score-level fusion technique to retain richer multimodal interactions. AV²A also includes a within-video label shift algorithm, leveraging input video data and predictions from prior frames to dynamically adjust event distributions for subsequent frames. Moreover, we present the first training-free, open-vocabulary baseline for audio-visual event perception, demonstrating that AV²A achieves substantial improvements over naive training-free baselines. We demonstrate the effectiveness of AV²A on both zero-shot and weakly-supervised state-of-the-art methods, achieving notable improvements in performance metrics over existing approaches.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Task Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3 has-text-centered">Task Overview</h2>
        <figure class="image" style="margin-top: 16px;">
          <img src="./static/images/task_figure.png" alt="Audio-Visual Event Perception Task">
        </figure>
        <p class="has-text-justified" style="margin-top: 12px;">
          <strong>Overview of the AVVP task.</strong> Audio-visual event perception focuses on predicting the temporal boundaries of events within a video that are exclusively visible (shown in blue), exclusively audible (shown in red), or both audible and visible (shown in purple).
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-10">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <figure class="image" style="margin-top: 16px;">
          <img src="./static/images/main_figure.png" alt="Method diagram">
        </figure>
        <div class="content has-text-justified" style="margin-top: 16px;">
          <p>
            <strong>Overview of AV<sup>2</sup>A.</strong> The process begins with category selection, where the input video clip passes through a video-level score-level fusion module (blue) to select relevant categories based on a threshold <em>&tau;<sub>f</sub></em>. These categories guide segment-level score-level fusion, where a dynamic threshold module (orange) updates thresholds <em>&tau;<sup>t</sup></em> via our label-shift technique, using the soft confusion matrix <em>M</em> from prior predictions <em>Y<sub>1</sub>, . . . , Y<sub>t&minus;1</sub></em> and segment scores <em>P<sup>1</sup><sub>av</sub>, . . . , P<sup>t&minus;1</sup><sub>av</sub></em>, cosine similarity between segments and <em>P<sup>t</sup><sub>av</sub></em>. Finally, predicted candidates are validated against a confidence threshold <em>&tau;<sub>r</sub></em>, retaining only those above it. This figure illustrates the process for audio-visual events; audio and visual events are handled similarly.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results (static images) -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>
    <div class="columns is-centered">
      <div class="column is-10">
        <figure class="image" style="margin-bottom: 16px;">
          <img src="./static/images/training_free_tab.png" alt="Training-free methods comparison">
        </figure>
        <p class="has-text-centered" style="margin-top: 8px; margin-bottom: 48px;">
          <strong>Table 1:</strong> Comparison of training-free methods on the LLP dataset, reporting AVVP metrics.
        </p>

        <figure class="image" style="margin-bottom: 16px; max-width: 50%; margin-left: auto; margin-right: auto;">
          <img src="./static/images/zs_baselines.png" alt="Zero-shot methods comparison">
        </figure>
        <p class="has-text-centered" style="margin-top: 8px;">
          <strong>Table 2:</strong> Comparison with zero-shot methods on AVE datasets, reporting AVEL metrics.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Qualitative Results -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
    <div class="columns is-centered">
      <div class="column is-10">
        <figure class="image" style="margin-bottom: 32px;">
          <img src="./static/images/wins&losses.png" alt="Qualitative analysis of AV2A">
        </figure>
        <p class="has-text-justified" style="margin-top: 12px;">
          <strong>Performance analysis of AV<sup>2</sup>A, based on the LanguageBind [37] model, showcasing predictions on audio-visual events, specifically those occurring simultaneously in both audio and video.</strong> Comparisons highlight (a) improvements and (b) failure-cases relative to state-of-the-art weakly supervised baselines.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@inproceedings{shaar2025adapting,
  title={Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds},
  author={Shaar, Eitan and Shaulov, Ariel and Chechik, Gal and Wolf, Lior},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={3142--3151},
  year={2025}
}
</code></pre>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://arxiv.org/abs/2503.13693.pdf"><i class="fas fa-file-pdf"></i></a>
      <a class="icon-link" href="https://github.com/eitan159/AV2A" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website template is adapted from the
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank" rel="noopener">Nerfies project page</a>.
            It is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>.
          </p>
          <p>
            Per their request, please keep a footer link back to the original template repository.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
